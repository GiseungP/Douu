from flask import Flask, render_template, request, jsonify, session, redirect, url_for
import numpy as np
import tensorflow as tf
import mediapipe as mp
import cv2
import base64
import time
import os
import uuid
import logging
import threading
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import random
import csv
import subprocess

app = Flask(__name__)
app.secret_key = 'your_secret_key_here'
app.config['UPLOAD_FOLDER'] = 'static/uploads'
app.config['SAVED_SENTENCES_FOLDER'] = 'saved_sentences'
app.config['DEBUG_FRAMES_FOLDER'] = 'debug_frames'
app.config['MODEL_FOLDER'] = 'model'
app.config['DATASET_FILE'] = 'korean_sign_frame_dataset.csv'

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('SignLanguageApp')

# í•„ìš”í•œ í´ë” ìƒì„±
for folder in [app.config['UPLOAD_FOLDER'], 
               app.config['SAVED_SENTENCES_FOLDER'], 
               app.config['DEBUG_FRAMES_FOLDER'],
               app.config['MODEL_FOLDER']]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# ëª¨ë¸ ë° í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
model = None
classes = []
current_model_version = 0
le = LabelEncoder()

# í•œê¸€ ìëª¨ ë¶„ë¥˜
CONSONANTS = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
VOWELS = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜', 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']

def load_latest_model():
    global model, classes, current_model_version, le
    
    # í´ë˜ìŠ¤ ë¡œë“œ ì‹œë„
    classes_path = os.path.join(app.config['MODEL_FOLDER'], 'korean_sign_frame_classes.npy')
    if os.path.exists(classes_path):
        classes = np.load(classes_path, allow_pickle=True)
        le.fit(classes)  # LabelEncoder ì´ˆê¸°í™”
        logger.info(f"âœ… í´ë˜ìŠ¤ ë¡œë“œ ì„±ê³µ! ê°œìˆ˜: {len(classes)}")
    else:
        logger.warning("âš ï¸ í´ë˜ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« í•™ìŠµ í›„ ìƒì„±ë©ë‹ˆë‹¤.")
    
    try:
        # ê°€ì¥ ìµœì‹  ëª¨ë¸ ì°¾ê¸°
        model_files = [f for f in os.listdir(app.config['MODEL_FOLDER']) 
                      if f.startswith('korean_sign_frame_model_v') and f.endswith('.h5')]
        
        if model_files:
            # ë²„ì „ ë²ˆí˜¸ë¡œ ì •ë ¬ (vìˆ«ì.h5)
            model_files.sort(key=lambda x: int(x.split('_v')[1].split('.')[0]), reverse=True)
            latest_model = model_files[0]
            current_model_version = int(latest_model.split('_v')[1].split('.')[0])
            
            model_path = os.path.join(app.config['MODEL_FOLDER'], latest_model)
            model = tf.keras.models.load_model(model_path)
            logger.info(f"âœ… ìµœì‹  ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (ë²„ì „: v{current_model_version})")
        else:
            logger.warning("âš ï¸ ì´ˆê¸° ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ í•™ìŠµ í›„ ìƒì„±ë©ë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

# ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
load_latest_model()

# MediaPipe ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ì›ë˜ í•™ìŠµ ì½”ë“œì˜ ë°ì´í„° ì¦ê°• í•¨ìˆ˜ (ë™ì¼í•˜ê²Œ ìœ ì§€)
def augment_frame_data(frame):
    """ì›ë˜ í•™ìŠµ ì½”ë“œì˜ ì¦ê°• í•¨ìˆ˜ì™€ ë™ì¼"""
    augmented_frames = []
    
    # ì›ë³¸ ë°ì´í„°
    augmented_frames.append(frame)
    
    # ì¢Œìš° ë°˜ì „ (x ì¢Œí‘œ ë°˜ì „)
    flipped = frame.copy()
    for i in range(0, len(flipped), 3):  # x ì¢Œí‘œë§Œ ì²˜ë¦¬
        flipped[i] = 1.0 - flipped[i]
    augmented_frames.append(flipped)
    
    # ì‘ì€ ì´ë™ ë³€í™˜
    for _ in range(3):  # 3ê°€ì§€ ì´ë™ ë²„ì „ ìƒì„±
        shifted = frame.copy()
        dx = random.uniform(-0.05, 0.05)  # x ì´ë™
        dy = random.uniform(-0.05, 0.05)  # y ì´ë™
        for i in range(0, len(shifted), 3):
            shifted[i] += dx   # x ì´ë™
            shifted[i+1] += dy  # y ì´ë™
        augmented_frames.append(shifted)
    
    # ìŠ¤ì¼€ì¼ ë³€í™˜
    scaled = frame.copy()
    scale_factor = random.uniform(0.9, 1.1)  # 10% í™•ëŒ€/ì¶•ì†Œ
    for i in range(0, len(scaled), 3):
        scaled[i] = 0.5 + (scaled[i] - 0.5) * scale_factor  # x ìŠ¤ì¼€ì¼
        scaled[i+1] = 0.5 + (scaled[i+1] - 0.5) * scale_factor  # y ìŠ¤ì¼€ì¼
    augmented_frames.append(scaled)
    
    # íšŒì „ ë³€í™˜ (ê°„ë‹¨í•œ 2D íšŒì „)
    angle = random.uniform(-15, 15) * np.pi / 180  # -15~15ë„ íšŒì „
    rotated = frame.copy()
    for i in range(0, len(rotated), 3):
        x = rotated[i] - 0.5
        y = rotated[i+1] - 0.5
        rotated[i] = 0.5 + x * np.cos(angle) - y * np.sin(angle)
        rotated[i+1] = 0.5 + x * np.sin(angle) + y * np.cos(angle)
    augmented_frames.append(rotated)
    
    return augmented_frames

def process_frame(frame, debug=False):
    if model is None:
        return None, 0
    
    try:
        # í”„ë ˆì„ í¬ê¸° ì¡°ì •: í•™ìŠµ ì‹œ ì‚¬ìš©í•œ 640x480ìœ¼ë¡œ ì¡°ì •
        frame = cv2.resize(frame, (640, 480))
        
        # í”„ë ˆì„ ì²˜ë¦¬ (RGB ë³€í™˜)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # MediaPipeë¡œ ì† ì¸ì‹
        results = hands.process(frame_rgb)
        
        prediction = None
        confidence = 0
        
        if results.multi_hand_landmarks:
            # ì²« ë²ˆì§¸ ì†ì˜ ëœë“œë§ˆí¬ ì‚¬ìš©
            hand_landmarks = results.multi_hand_landmarks[0]
            
            # ë””ë²„ê·¸: ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            if debug:
                debug_frame = frame.copy()
                mp_drawing.draw_landmarks(
                    debug_frame, 
                    hand_landmarks, 
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)
                )
                # ë””ë²„ê·¸ í”„ë ˆì„ ì €ì¥
                timestamp = int(time.time() * 1000)
                debug_frame_path = os.path.join(app.config['DEBUG_FRAMES_FOLDER'], f"frame_{timestamp}.jpg")
                cv2.imwrite(debug_frame_path, debug_frame)
                logger.info(f"ğŸ“¸ ë””ë²„ê·¸ í”„ë ˆì„ ì €ì¥: {debug_frame_path}")
            
            # í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ëœë“œë§ˆí¬ ì¶”ì¶œ
            frame_data = []
            for lm in hand_landmarks.landmark:
                frame_data.extend([lm.x, lm.y, lm.z])
            
            # ëœë“œë§ˆí¬ ê°œìˆ˜ í™•ì¸ (21ê°œ ëœë“œë§ˆí¬ * 3 = 63)
            if len(frame_data) != 63:
                logger.warning(f"âš ï¸ ëœë“œë§ˆí¬ ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(frame_data)} (ê¸°ëŒ€ê°’: 63)")
                return None, 0
            
            # ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë°°ì—´ ì¬êµ¬ì„±
            input_data = np.array(frame_data, dtype=np.float32).reshape(1, -1)
            
            # ì…ë ¥ ì°¨ì› ê²€ì¦ (63 ê³ ì •)
            if input_data.shape[1] != 63:
                logger.error(f"âŒ ì…ë ¥ ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ëŒ€ê°’ 63 vs ì…ë ¥ {input_data.shape[1]}")
                return None, 0
            
            # ëª¨ë¸ ì˜ˆì¸¡
            predictions = model.predict(input_data, verbose=0)
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
            pred_idx = np.argmax(predictions[0])
            confidence = float(np.max(predictions[0]))
            
            # í´ë˜ìŠ¤ ë§¤í•‘ í™•ì¸
            if classes.size > 0 and 0 <= pred_idx < len(classes):
                prediction = classes[pred_idx]
                logger.info(f"ğŸ” ì¸ì‹ ì„±ê³µ: {prediction} (ì‹ ë¢°ë„: {confidence:.2f})")
            else:
                logger.warning(f"âš ï¸ ì¸ë±ìŠ¤ ì˜¤ë¥˜: pred_idx={pred_idx}, í´ë˜ìŠ¤ ê°œìˆ˜={len(classes)}")
                prediction = None
                confidence = 0
        
        else:
            logger.info("ğŸ–ï¸ ì†ì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            if debug:
                timestamp = int(time.time() * 1000)
                debug_frame_path = os.path.join(app.config['DEBUG_FRAMES_FOLDER'], f"no_hand_{timestamp}.jpg")
                cv2.imwrite(debug_frame_path, frame)
                logger.info(f"ğŸ“¸ ì† ë¯¸ì¸ì‹ í”„ë ˆì„ ì €ì¥: {debug_frame_path}")
        
        return prediction, confidence
    
    except Exception as e:
        logger.error(f"âŒ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return None, 0

# ë¼ìš°íŠ¸ ì •ì˜
@app.route('/')
def index():
    session['user_id'] = str(uuid.uuid4())
    session['sentence'] = ""  # ë¬¸ìì—´ë¡œ ë¬¸ì¥ ì €ì¥
    return render_template('index.html')

@app.route('/sentence')
def sentence_page():
    if 'sentence' not in session:
        session['sentence'] = ""
    return render_template('sentence.html', sentence=session['sentence'])

@app.route('/add_sign')
def add_sign_page():
    return render_template('add_sign.html')

# API ì—”ë“œí¬ì¸íŠ¸
@app.route('/recognize', methods=['POST'])
def recognize():
    if model is None:
        return jsonify({
            'prediction': None,
            'confidence': 0,
            'error': 'ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨'
        })
    
    data = request.get_json()
    if 'image' not in data:
        return jsonify({'error': 'ì´ë¯¸ì§€ ë°ì´í„° ì—†ìŒ'})
    
    try:
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        header, encoded = data['image'].split(",", 1)
        nparr = np.frombuffer(base64.b64decode(encoded), np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if frame is None:
            logger.error("âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")
            return jsonify({'error': 'ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨'})
        
        # í”„ë ˆì„ ì²˜ë¦¬ (ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”)
        prediction, confidence = process_frame(frame, debug=True)
        
        # ììŒ/ëª¨ìŒë§Œ ë°˜í™˜
        if prediction and (prediction in CONSONANTS or prediction in VOWELS):
            return jsonify({
                'prediction': prediction,
                'confidence': confidence
            })
        else:
            return jsonify({
                'prediction': None,
                'confidence': 0
            })
    
    except Exception as e:
        logger.error(f"âŒ ì¸ì‹ ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'error': str(e)})

@app.route('/add_to_sentence', methods=['POST'])
def add_to_sentence():
    if 'sentence' not in session:
        session['sentence'] = ""
    
    char = request.json.get('char')
    if char:
        # ìƒˆ ë¬¸ì ì¶”ê°€
        new_sentence = session['sentence'] + char
        session['sentence'] = new_sentence
        
        return jsonify({
            'success': True,
            'sentence': new_sentence
        })
    
    return jsonify({'success': False, 'error': 'ë¬¸ì ì—†ìŒ'})

@app.route('/reset_sentence', methods=['POST'])
def reset_sentence():
    session['sentence'] = ""
    return jsonify({
        'success': True,
        'sentence': ''
    })

@app.route('/get_sentence', methods=['GET'])
def get_sentence():
    current_sentence = session.get('sentence', "")
    return jsonify({
        'sentence': current_sentence
    })

@app.route('/save_sentence', methods=['POST'])
def save_sentence():
    current_sentence = session.get('sentence', "")
    
    if not current_sentence:
        return jsonify({'success': False, 'error': 'ì €ì¥í•  ë¬¸ì¥ ì—†ìŒ'})
    
    try:
        save_folder = app.config['SAVED_SENTENCES_FOLDER']
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"sentence_{timestamp}_{session['user_id'][:8]}.txt"
        filepath = os.path.join(save_folder, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(current_sentence)
        
        logger.info(f"ğŸ’¾ ë¬¸ì¥ ì €ì¥: {filepath}")
        return jsonify({'success': True, 'filename': filename})
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì¥ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})

# ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° í•™ìŠµ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ë™ì¼í•˜ê²Œ ìœ ì§€)
def extract_frames_from_video(video_path, target_frames=30):
    """ë™ì˜ìƒì—ì„œ ëŒ€í‘œ í”„ë ˆì„ ì¶”ì¶œ (webm ì§ì ‘ ì§€ì›)"""
    logger.info(f"ğŸ“¹ í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘: {video_path}")
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    if not cap.isOpened():
        logger.error(f"âŒ ë™ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        return frames
    
    try:
        # ë™ì˜ìƒì—ì„œ í”„ë ˆì„ ìƒ˜í”Œë§
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"ì´ í”„ë ˆì„ ìˆ˜: {total_frames}, FPS: {fps}")
        
        if total_frames < 1 or fps <= 0:
            logger.warning("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë™ì˜ìƒ ì •ë³´, ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ì „í™˜")
            # ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹: 1ì´ˆë‹¹ 1í”„ë ˆì„
            fps = 30  # ê¸°ë³¸ FPS ê°’
            total_frames = 0
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                total_frames += 1
                # 1ì´ˆë‹¹ 1í”„ë ˆì„ ì¶”ì¶œ
                if total_frames % int(fps) == 0:
                    frame = cv2.resize(frame, (640, 480))
                    frames.append(frame)
            logger.info(f"ğŸ”„ ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ: {len(frames)}ê°œ")
            return frames
    
    except Exception as e:
        logger.error(f"âŒ ë™ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ì „í™˜
        total_frames = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            total_frames += 1
            # 1ì´ˆë‹¹ 1í”„ë ˆì„ ì¶”ì¶œ (30 FPS ê°€ì •)
            if total_frames % 30 == 0:
                frame = cv2.resize(frame, (640, 480))
                frames.append(frame)
        logger.info(f"ğŸ”„ ì˜ˆì™¸ í›„ ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ: {len(frames)}ê°œ")
        return frames
    
    try:
        # í”„ë ˆì„ ìƒ˜í”Œë§ ì „ëµ ê°œì„ 
        if total_frames < target_frames:
            # í”„ë ˆì„ ìˆ˜ê°€ ì ìœ¼ë©´ ëª¨ë“  í”„ë ˆì„ ì‚¬ìš©
            frame_indices = range(0, total_frames)
        else:
            # ê³ ë¥´ê²Œ ë¶„í¬ëœ í”„ë ˆì„ ì„ íƒ
            frame_indices = np.linspace(0, total_frames-1, target_frames, dtype=int)
        
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                # í”„ë ˆì„ ì „ì²˜ë¦¬ (í¬ê¸° ì¡°ì •: í•™ìŠµê³¼ ë™ì¼í•œ 640x480)
                frame = cv2.resize(frame, (640, 480))
                frames.append(frame)
            else:
                logger.warning(f"âš ï¸ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨: ì¸ë±ìŠ¤ {idx}")
    
    except Exception as e:
        logger.error(f"âŒ í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ì „í™˜
        cap.release()
        cap = cv2.VideoCapture(video_path)
        total_frames = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            total_frames += 1
            # 1ì´ˆë‹¹ 1í”„ë ˆì„ ì¶”ì¶œ (30 FPS ê°€ì •)
            if total_frames % 30 == 0:
                frame = cv2.resize(frame, (640, 480))
                frames.append(frame)
        logger.info(f"ğŸ”„ ì˜ˆì™¸ í›„ ê¸°ë³¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ: {len(frames)}ê°œ")
    
    finally:
        cap.release()
    
    logger.info(f"âœ… ì¶”ì¶œëœ í”„ë ˆì„ ìˆ˜: {len(frames)}")
    return frames

def extract_landmarks(frame):
    """í”„ë ˆì„ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ (í•™ìŠµ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ)"""
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(frame_rgb)
    
    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        frame_data = []
        for lm in hand_landmarks.landmark:
            frame_data.extend([lm.x, lm.y, lm.z])
        return frame_data
    return None

def create_simple_mlp_model(input_shape, num_classes):
    """ì›ë˜ í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°"""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(256, activation='relu', input_shape=input_shape),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.4),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

def update_dataset(new_data, label):
    """CSV ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸ (ì›ë˜ í•™ìŠµ í¬ë§· ìœ ì§€)"""
    dataset_path = app.config['DATASET_FILE']
    
    # ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    if os.path.exists(dataset_path):
        df = pd.read_csv(dataset_path, encoding='utf-8-sig')
    else:
        # ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
        columns = [f'{ax}_{i}' for i in range(21) for ax in ['x', 'y', 'z']] + ['label']
        df = pd.DataFrame(columns=columns)
    
    # ìƒˆ ë°ì´í„° ì¶”ê°€
    for data in new_data:
        row = list(data) + [label]
        df.loc[len(df)] = row
    
    # CSV ì €ì¥
    df.to_csv(dataset_path, index=False, encoding='utf-8-sig')
    logger.info(f"ğŸ’¾ ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸: {dataset_path} (ìƒˆ ìƒ˜í”Œ: {len(new_data)}ê°œ)")

def retrain_model_with_new_data(video_path, label, category):
    """ìƒˆ ë™ì˜ìƒ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ (webm ì§ì ‘ ì§€ì›)"""
    global model, classes, current_model_version, le
    
    try:
        logger.info(f"ğŸš€ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘: {label} ({category})")
        
        # 1. ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ (ë³€í™˜ ì—†ì´ ì§ì ‘ ì²˜ë¦¬)
        frames = extract_frames_from_video(video_path, target_frames=30)
        
        if not frames:
            logger.error("âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨ - ë¹ˆ í”„ë ˆì„ ëª©ë¡")
            return False
            
        logger.info(f"ğŸ“¹ ì¶”ì¶œëœ í”„ë ˆì„ ìˆ˜: {len(frames)}")
            
        # 2. í”„ë ˆì„ ì²˜ë¦¬ ë° ëœë“œë§ˆí¬ ì¶”ì¶œ + ì¦ê°•
        new_data = []
        valid_frames = 0
        
        for i, frame in enumerate(frames):
            landmarks = extract_landmarks(frame)
            if landmarks and len(landmarks) == 63:
                valid_frames += 1
                # ì›ë˜ í•™ìŠµ ì½”ë“œì˜ ì¦ê°• ì ìš©
                augmented_frames = augment_frame_data(landmarks)
                new_data.extend(augmented_frames)
            else:
                logger.warning(f"âš ï¸ í”„ë ˆì„ {i+1}ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨")
        
        logger.info(f"ğŸ” ìœ íš¨í•œ í”„ë ˆì„ ìˆ˜: {valid_frames}/{len(frames)}")
        
        if not new_data:
            logger.error("âŒ ìœ íš¨í•œ ëœë“œë§ˆí¬ ë°ì´í„° ì—†ìŒ")
            return False
        
        # 3. ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸ (CSV íŒŒì¼)
        update_dataset(new_data, label)
        
        # 4. ì „ì²´ ë°ì´í„°ì…‹ ì¬ë¡œë“œ
        dataset_path = app.config['DATASET_FILE']
        if not os.path.exists(dataset_path):
            logger.error("âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
            
        df = pd.read_csv(dataset_path, encoding='utf-8-sig')
        X = df.iloc[:, :-1].values.astype(np.float32)
        y = df['label'].values
        
        # 5. í´ë˜ìŠ¤ ì—…ë°ì´íŠ¸
        classes = np.unique(y)
        np.save(os.path.join(app.config['MODEL_FOLDER'], 'korean_sign_frame_classes.npy'), classes)
        le.fit(classes)
        
        # 6. ë ˆì´ë¸” ì¸ì½”ë”©
        y_encoded = le.transform(y)
        y_categorical = to_categorical(y_encoded, num_classes=len(classes))
        
        # 7. ë°ì´í„° ë¶„í•  (ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ)
        # ì¬í•™ìŠµ ì‹œì—ëŠ” ì „ì²´ ë°ì´í„° ì‚¬ìš©
        X_train, X_val, y_train, y_val = train_test_split(
            X, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        # 8. ëª¨ë¸ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
        if model is None:
            logger.info("ğŸ†• ìƒˆ ëª¨ë¸ ìƒì„± (ì›ë˜ í•™ìŠµ êµ¬ì¡°)")
            new_model = create_simple_mlp_model(
                input_shape=(X_train.shape[1],),
                num_classes=len(classes))
        else:
            # ê¸°ì¡´ ëª¨ë¸ êµ¬ì¡° ë³µì œ
            logger.info("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë³µì œ")
            new_model = create_simple_mlp_model(
                input_shape=(X_train.shape[1],),
                num_classes=len(classes))
        
        # 9. ëª¨ë¸ í•™ìŠµ (ì›ë˜ í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼)
        logger.info(f"ğŸ” ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘: {X_train.shape[0]}ê°œ ìƒ˜í”Œ")
        history = new_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=32,
            verbose=0
        )
        
        # 10. ìƒˆ ëª¨ë¸ ì €ì¥
        current_model_version += 1
        new_model_path = os.path.join(app.config['MODEL_FOLDER'], f'korean_sign_frame_model_v{current_model_version}.h5')
        new_model.save(new_model_path)
        
        # 11. ëª¨ë¸ êµì²´
        model = new_model
        logger.info(f"âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ë²„ì „: v{current_model_version}")
        logger.info(f"í˜„ì¬ í´ë˜ìŠ¤ ê°œìˆ˜: {len(classes)}")
        
        return True
    
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì¬í•™ìŠµ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return False

# ìˆ˜ì–´ ì¶”ê°€ API ì—”ë“œí¬ì¸íŠ¸ (ê°œì„ ëœ ë²„ì „)
@app.route('/add_sign', methods=['POST'])
def add_sign():
    try:
        # í•„ìˆ˜ í•­ëª©: ë‹¨ì–´ì™€ íŒŒì¼
        word = request.form.get('word')
        file = request.files.get('file')
        
        # ì„ íƒ í•­ëª© (ê¸°ë³¸ê°’ ì„¤ì •)
        meaning = request.form.get('meaning', 'ì¶”ê°€ ì˜ˆì •')
        category = request.form.get('category', 'ê¸°íƒ€')
        description = request.form.get('description', 'ì¶”ê°€ ì˜ˆì •')
        
        # í•„ìˆ˜ í•­ëª© ê²€ì¦
        if not word or not file:
            return jsonify({
                'success': False, 
                'error': 'ìˆ˜ì–´ ë‹¨ì–´ì™€ ë™ì˜ìƒ íŒŒì¼ì€ í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤'
            })
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        filename = file.filename
        _, ext = os.path.splitext(filename)
        supported_formats = ['.mp4', '.avi', '.mov', '.webm']
        
        if ext.lower() not in supported_formats:
            return jsonify({
                'success': False, 
                'error': f'ì§€ì›í•˜ëŠ” ë™ì˜ìƒ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {", ".join(supported_formats)}'
            })
        
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        new_filename = f"sign_{int(time.time())}_{uuid.uuid4().hex[:6]}{ext}"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], new_filename)
        file.save(filepath)
        logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥: {filepath}")
        
        # ë¹„ë™ê¸°ë¡œ ì¬í•™ìŠµ ì‹œì‘
        threading.Thread(
            target=retrain_model_with_new_data,
            args=(filepath, word, category)
        ).start()
        
        return jsonify({
            'success': True,
            'message': 'ìˆ˜ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤! ëª¨ë¸ ì¬í•™ìŠµì´ ì‹œì‘ë©ë‹ˆë‹¤.',
            'data': {
                'word': word,
                'meaning': meaning,
                'category': category,
                'description': description,
                'file_path': filepath
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ ìˆ˜ì–´ ë“±ë¡ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False, 
            'error': str(e)
        })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)